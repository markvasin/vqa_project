project_name: clevr_project

optimizer:
  type: adam_w
  params:
    lr: 3e-4
    eps: 1e-8

#scheduler:
#  type: warmup_linear
#  params:
#    num_warmup_steps: 2000
#    num_training_steps: 88000
env:
  user_dir: /home/mark/vqa_project
  save_dir: ./save/${training.experiment_name}


evaluation:
  metrics:
  - accuracy

training:
  batch_size: 128
  lr_scheduler: false
#   Don't forget to update schedule_attributes if you update this
  max_updates: 88000
  evaluation_interval: 2000
  checkpoint_interval: 1000
#  early_stop:
#    criteria: gqa2/vqa_accuracy
#    minimize: false
  tensorboard: true
  wandb: true

#checkpoint:
#  pretrained_state_mapping:
#    bert: bert

#dataset_config:
#  clevr:
#    processors:
#      image_processor:
#        type: torchvision_transforms
#        params:
#          transforms:
#            - type: Resize
#              params:
#                size: [224, 224]
#            - ToTensor
#            - type: Normalize
#              params:
#                mean: [0.485, 0.456, 0.406]
#                std: [0.229, 0.224, 0.225]
#      text_processor:
#        type: vocab
#        params:
#          max_length: 30
#          vocab:
#            type: intersected
#            embedding_name: glove.6B.300d
#            vocab_file: vocabs/clevr_question_vocab.txt
#          preprocessor:
#            type: simple_sentence
#            params: {}

model_config:
  vqa_transformer:
    training_head_type: classification
    num_labels: 32
    losses:
      - type: cross_entropy
    image_hidden_size: 1024
    num_segment_type: 2
    hidden_size: 256
    num_hidden_layers: 6
    num_attention_heads: 4
    intermediate_size: 1024
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    layer_norm_eps: 1e-12

    text_embedding:
      embedding_dim: 300
    lstm:
      input_size: 300
      hidden_size: ${model_config.vqa_transformer.hidden_size}
      bidirectional: true
      batch_first: true